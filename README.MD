# TRM reproduction

I went with the Sudoku-Hard, because it requires little compute while providing an interesting insight into the MLP-mixer vs self-attention mechanisms.
I ran all of it on an RTX A6000. Attention took 17h 47m, MLP 22h 39m. Both converged nicely without any hickups (which is quite rare out of the box for a research publication - solid job on Alexia Jolicoeur-Martineau's part, that girl knows her stuff).

